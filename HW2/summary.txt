########################
# Additional Files
########################
# .DS_Store
# readme.md

########################
# Filled Code
########################
# ../codes/cnn/model.py:1
    def __init__(self, num_features, momentum, eps):
        super(BatchNorm2d, self).__init__()
        self.weight = Parameter(torch.ones(num_features))
        self.bias = Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.momentum = momentum
        self.eps = eps
        # input: [batch_size, num_feature_map, height, width]
        if self.training:
            batch_mean = torch.mean(input, dim=[0, 2, 3])
            batch_var = torch.var(input, dim=[0, 2, 3])
            self.running_mean = (1 - self.momentum) * self.running_mean.data + self.momentum * batch_mean
            self.running_var =  (1 - self.momentum) * self.running_var.data + self.momentum * batch_var
        else:
            batch_mean = self.running_mean.data
            batch_var = self.running_var.data
        normalized_output = (input - batch_mean[:, None, None]) / torch.sqrt(batch_var[:, None, None] + self.eps)
        normalized_output = self.weight[:, None, None] * normalized_output + self.bias[:, None, None]

        return normalized_output

# ../codes/cnn/model.py:2
        # input: [batch_size, num_feature_map, height, width]
        if self.training:
            output = torch.bernoulli(torch.ones_like(input), (1 - self.p)) * input
            output /= (1 - self.p)
        else:
            output = input

        return output

# ../codes/cnn/model.py:3
        self.layers = nn.Sequential(
            nn.Conv2d(3, 128, 5),
            BatchNorm2d(128, 0.1, 1e-5),
            nn.ReLU(),
            Dropout(drop_rate),
            nn.MaxPool2d(3, 2),
            nn.Conv2d(128, 64, 3),
            BatchNorm2d(64, 0.1, 1e-5),
            nn.ReLU(),
            Dropout(drop_rate),
            nn.MaxPool2d(3, 2)
        )
        self.linear = nn.Linear(1600, 10)

# ../codes/cnn/model.py:4
        logits = self.layers(x)
        logits = torch.reshape(logits, (logits.shape[0], -1))
        logits = self.linear(logits)

# ../codes/mlp/model.py:1
    def __init__(self, num_features, momentum, eps):
        self.num_features = num_features # hidden layer neutral number
        self.weight = Parameter(torch.ones(num_features))
        self.bias = Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.momentum = momentum
        self.eps = eps
        if self.training:
            batch_mean = torch.mean(input, dim=0)
            batch_var = torch.var(input, dim=0)
            # exponential moving average
            self.running_mean = (1 - self.momentum) * self.running_mean.data + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var.data + self.momentum * batch_var
        else:
            batch_mean = self.running_mean.data
            batch_var = self.running_var.data

        normalized_output = (input - batch_mean) / torch.sqrt(batch_var + self.eps)
        normalized_output = self.weight * normalized_output + self.bias

        return normalized_output

# ../codes/mlp/model.py:2
        if self.training:
            output = torch.bernoulli(torch.ones_like(input), (1 - self.p)) * input
            output /= (1 - self.p)
        else:
            output = input

        return output

# ../codes/mlp/model.py:3
        self.layers = nn.Sequential(
            nn.Linear(3 * 32 * 32, 512),
            BatchNorm1d(512, 0.1, 1e-5),
            nn.ReLU(),
            Dropout(drop_rate),
            nn.Linear(512, 10)
        )

# ../codes/mlp/model.py:4
        logits = self.layers(x)


########################
# References
########################

########################
# Other Modifications
########################
# _codes/cnn/model.py -> ../codes/cnn/model.py
# 7 - class BatchNorm1d(nn.Module):
# 7 ?                ^
# 7 + class BatchNorm2d(nn.Module):
# 7 ?                ^
# 47 -     def forward(self, x, y=None):
# 79 +     def forward(self, x, y=None):
# 79 ?                                  +
# _codes/cnn/main.py -> ../codes/cnn/main.py
# 11 + # from torch.utils.tensorboard import SummaryWriter
# 17 + # writer = SummaryWriter('log')
# 123 -
# 125 +             # writer.add_scalars(f'train_loss', {f'cnnWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': train_loss}, epoch)
# 126 +             # writer.add_scalars(f'train_acc', {f'cnnWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': train_acc}, epoch)
# 128 +             # writer.add_scalars(f'val_loss', {f'cnnWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': val_loss}, epoch)
# 129 +             # writer.add_scalars(f'val_acc', {f'cnnWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': val_acc}, epoch)
# 130 -                 with open(os.path.join(args.train_dir, 'checkpoint_{}.pth.tar'.format(epoch)), 'wb') as fout:
# 135 +                 # with open(os.path.join(args.train_dir, 'checkpoint_{}.pth.tar'.format(epoch)), 'wb') as fout:
# 135 ?                ++
# 131 -                     torch.save(cnn_model, fout)
# 131 ?                  ^^^
# 136 +                 # 	torch.save(cnn_model, fout)
# 136 ?                 + ^
# 132 -                 with open(os.path.join(args.train_dir, 'checkpoint_0.pth.tar'), 'wb') as fout:
# 137 +                 # with open(os.path.join(args.train_dir, 'checkpoint_0.pth.tar'), 'wb') as fout:
# 137 ?                ++
# 133 -                     torch.save(cnn_model, fout)
# 133 ?                  ^^^
# 138 +                 # 	torch.save(cnn_model, fout)
# 138 ?                 + ^
# 153 -         print("begin testing")
# 164 -             test_image = X_test[i].reshape((1, 3, 32, 32))
# 164 ?                                                 ^   ^
# 168 +             test_image = X_test[i].reshape((1, 3 * 32 * 32))
# 168 ?                                                 ^^   ^^
# _codes/mlp/main.py -> ../codes/mlp/main.py
# 11 + # from torch.utils.tensorboard import SummaryWriter
# 17 + # writer = SummaryWriter('log')
# 108 -         mlp_model = Model(drop_rate=drop_rate)
# 110 +         mlp_model = Model(drop_rate=args.drop_rate)
# 110 ?                                     +++++
# 113 -         # model_path = os.path.join(args.train_dir, 'checkpoint_%d.pth.tar' % args.inference_version)
# 113 ?        --
# 115 +         model_path = os.path.join(args.train_dir, 'checkpoint_%d.pth.tar' % args.inference_version)
# 123 -
# 125 +             # writer.add_scalars(f'train_loss', {f'mlpWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': train_loss}, epoch)
# 126 +             # writer.add_scalars(f'train_acc', {f'mlpWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': train_acc}, epoch)
# 128 +             # writer.add_scalars(f'val_loss', {f'mlpWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': val_loss}, epoch)
# 129 +             # writer.add_scalars(f'val_acc', {f'mlpWD_{args.batch_size}_{args.learning_rate}_{args.drop_rate}': val_acc}, epoch)

